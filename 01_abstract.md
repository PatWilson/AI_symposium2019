# Full Abstract For Your Pocket

## An evaluation framework to test effectiveness of Artificial Intelligence/Machine learning (AI/ML) models in clinical practice

Patrick Wilson MPH, Curt Storlie PhD, Jacob Strand MD, Jon Ebbert MD MSc, Lindsey  Philpot PhD MPH, Priya Ramar MPH, Meredith DeZutter, Shusaku Asai BS, Dennis Murphree PhD, Daniel Quest PhD, Piyush Mukherjee BE, David Mead MS, Yaxiong Lin MS, Ing Tiong MS MA, Andrew Harrison MD PhD, Jalal Soleimani MD, Vitaly Herasevich MD PhD, Katherine Heise APRN CNP MSN, Joy Allen  APRN CNP, Brian Pickering MBBCh


With the growth of Artificial Intelligence/Machine learning (AI/ML) methods in healthcare, providers are poised to leverage vast amounts of data in the Electronic Medical Record (EMR) for the benefit of patients. Despite this increased enthusiasm, most research projects do not result in a production ready model that can be integrated into clinical workflows. Likewise, “success” is often defined as good external performance or comparison with annotated clinician judgement. For AI/ML to be successful with practicing clinicians, algorithms will require the testing framework principally used to test other clinical interventions used in practice, i.e., randomized clinical trials demonstrating benefit to patients.
However, the higher standard of demonstrating effectiveness is furthered compounded with the complexities of having to integrate AI/ML into practice. Researchers along with clinical partners need to take a fully functioning prediction model and assess workflows in order to develop a care model that integrates the AI/ML model into practice. This process requires a design that can balance these implementation concerns with scientific validity.
We present the integration of AI/ML as a problem solved by pragmatic clinical trials testing the production ready model with “real-world” data compared to the usual source of care.
In our flagship project we plan to test a fully developed decision support tool, known as the Control Tower, which is centered on an AI/ML risk score predicting patient need for a palliative care consultation during an inpatient stay. Success will be determined if patients in the intervention arm receive palliative care sooner than standard care here at the Mayo Clinic.
We will use a stepped-wedge cluster randomized trial at both hospitals in the Mayo Clinic. In the stepped-wedge design, clusters cross over randomly from the control condition to the intervention condition in a staggered fashion. The stepped wedge design is a pragmatic clinical trial design which has a number of benefits in testing interventions in “real-world” settings and works well with how clinical practice typically disseminates interventions, albeit in an experimental design. 
For units in the intervention arm, the results of the prediction model will be presented through the Control Tower graphical user interface (GUI). The intervention will include an operator who will monitor the GUI daily and select a cohort of patients with the highest need of palliative care review. The final list of patients will be sent to the palliative care team, who will assess the need for each patient, and approach the attending clinical team to suggest a referral to see the patient. 
Our approach leverages building a Bayesian hierarchical model and deriving the necessary power through Monte Carlo simulation. This approach permits flexible estimation to meet pragmatic constraints, and allows for a straightforward process to integrate pilot data derived from algorithm training and development. 
The need for evaluation of AI/ML models in healthcare is expected to increase, and the AI/ML community will benefit from engaging researchers well versed in implementation science, health services research and pragmatic clinical trials to demonstrate the effectiveness of their tools they develop.